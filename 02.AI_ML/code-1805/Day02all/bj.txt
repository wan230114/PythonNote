机器学习
一、概述
1.什么是机器学习
机器学习是一门能够让编程计算机从数据中学习的计算机科学。
机器学习研究如何让计算机不需要明确的程序也能具备学习能力。
一个计算机程序在完成某种任务之后，获得经验，该经验可以通过一个量化指标加以描述，如果该经验可以随着任务的丰富而不断提升，那么这样的计算机系统就被称为机器学习系统。
2.为什么需要机器学习
1)模式识别的过程可以自动完成，自动更新，易于维护，而且更加准确。
2)针对那些使用传统方法太过复杂，甚至根本不存在已知算法的问题。
3)帮助人类发现隐藏在数据中的规则――数据挖掘。
3.机器学习系统的种类
1)有监督学习、无监督学习、半监督学习和强化学习
输入  输出
a       1
b      1
c      0
d     1
e     0
f      1
.
.
.
x     ?
f(邮件)->是否垃圾
2)批量学习和在线学习
3)基于实例的学习和基于模型的学习
4.机器学习的挑战
1)训练数据不足
2)训练数据不具代表性
3)训练数据质量不佳
4)混在无关特征
5)过拟合和欠拟合
5.测试和验证
数据集--训练集
           \ 验证集
           \ 测试集
6.机器学习和人工智能
人工智能
       机器学习
              传统机器学习：基于标准算法，中小数据量
              深度学习：基于深度神经网络，海量数据量
       ...
7.机器学习的标准流程
1)数据采集
2)数据清洗
3)数据预处理
4)特征提取
5)选择模型(算法)
6)训练模型 <-+
7)验证模型 ---+
8)测试模型
9)使用模型
二、数据预处理
sci-kit learn
sklearn
nltk
opencv
nerolib
...
输入样本矩阵：一行一样本，一列一特征。
姓名 年龄 学历 -> 薪资
张三 25   本科       8000
李四 30   硕士       9000
王五 23   大专       10000
...
输出标签数组：对应于输入样本矩阵中的每个样本
[8000 9000 10000 ...]
1.均值移除
把输入样本矩阵的每个特征的平均值置为0，标准差置为1。
a
b
c
m = (a+b+c)/3
a' = a - m
b' = b - m
c' = c - m
m' = (a'+b'+c')/3
     = ((a-m)+(b-m)+(c-m))/3
     = ((a+b+c)-3m) / 3
     = 0
a'
b'
c'
s'^2 = (a'^2+b'^2+c'^2) / 3
a" = a'/s'
b" = b'/s'
c" = c'/s'
s"^2 = (a"^2+b"^2+c"^2)/3
          = (a'^2+b'^2+c'^2)/s'^2/3
          = 1
import sklearn.preprocessing as sp
sp.scale(原始样本矩阵)->均值移除后的样本矩阵
代码:std.py
2.范围缩放
        一班  二班 ...
数学   90     60
英语
...
将输入样本矩阵中的每个特征线性缩放到统一的最值区间。
x k + b = y
min k + b = 0
max k + b = 1
范围缩放器 = sp.MinMaxScaler(
    feature_range=(目标最小值, 目标最大值))
范围缩放器.fit_transform(原始样本矩阵)
    ->范围缩放后的样本矩阵
代码：mms.py
3.归一化
          Python C++ Java PHP
2015 10          20    30    15
2016 20          15    15    15
2017 ...
用每一个输入样本中的特征值除以该样本所有特征值的绝对值之和。
sp.normalize(原始样本矩阵, norm='l1')
    ->归一化后的样本矩阵
代码:nor.py
4.二值化
将输入样本矩阵中大于某个给定阈值的元素设定为1,其它元素均为0.
100 90 70 80 40 50 60
60
1 1 1 1 0 0 0
二值化器 = sp.Binarizer(threshold=阈值)
二值化器.transform(原始样本矩阵)
    ->二值化后的样本矩阵
代码:bin.py
5.独热编码
1            3            2
7            5            4
1            8            6
7            3            9
1: 10     3:100     2:1000
7: 01     5:010     4:0100
              8:001     6:0010
                             9:0001
101001000
010100100
100010010
011000001
独热编码器 = sp.OneHotEncoder(
    sparse=[是]否紧凑, dtype=类型)
独热编码器.fit_transform(原始样本矩阵)
    ->独热编码后的样本矩阵
代码:ohe.py
6.标签编码
按照字典顺序对每个特征中的文本标签进行编码,变成整数序列.
标签编码器 = sp.LabelEncoder()
标签编码器.fit_transform(标签序列)->编码序列
标签编码器.inverse_transform(编码序列)->标签序列
代码:lab.py
三、线性回归
x -> y
x -> y
x -> y
.
.
.
x -> ?
预测函数: y' = w0 + w1x
                           SIGMA[(y-(w0+w1x))^2]
损失函数: loss = -----------------------------
                                                 2
寻找合适的模型参数w0和w1使得损失函数的值最小.
dloss
------ = SIGMA[2(y-(w0+w1x))(-1)/2]
dw0   = -SIGMA[y-(w0+w1x)]
dloss
------ = SIGMA[2(y-(w0+w1x))(-x)/2]
dw1  = -SIGMA[(y-(w0+w1x))x]
w0 = w0 - ndloss/dwo
w1 = w1 - ndloss/dw1
代码:bgd.py
x1 x2 x3 ... xn -> y
y = w0 + w1x1 + w2x2 + w3x3 + ... + wnxn
import sklearn.linear_model as lm
线性回归器 = lm.LinearRegression()
线性回归器.fit(输入样本矩阵, 输出标签数组)
线性回归器.predict(输入样本矩阵)->预测的输出标签数组
------------------------------------------------------------------
代码：line.py
保存和载入模型
import pickle
pickle.dump(内存对象, 文件对象)
pickle.load(文件对象)->内存对象
代码：dump.py、load.py
四、岭回归
loss' = loss(w0, w1) - 正则强度 x 正则项(x, y)
岭回归就是在线性回归的基础上增加了正则项，有意破坏模型对训练数据集的拟合效果，客观上降低了少数异常样本对模型的牵制作用，使得模型对大多数正常样本表现出更好的拟合效果。
岭回归器 = lm.Ridge(alpha = 正则强度)
代码：rdg.py
五、多项式回归
y' = w0 + w1x + w2x^2 + w3x^3 + ... + wnx^n
x1 = x
x2 = x^2
x3 = x^3
...
xn = x^n
y' = w0 + w1x1 + w2x2 + w3x3 + ... + wnxn
x->多项式特征扩展器-x1...xn->线性回归器->w0...wn
       \____________________________________/
                                    |
                         管线(流水线)
代码：poly.py
六、决策树
回归问题：输出标签分布于无限连续域。
分类问题：输出标签分布于有限离散域。
核心思想：相似的因导致相似的果。
驾龄    均速   ->   性别
1         60             m
2         40             w
3         80             m
1         40             w
2         80             m
1         40             w
1         40             m
3         60             w
2         60             m
3         80             w
------------------------------------------------------------------
1 60 m   2 40 w    3 80  m
1 40 w   2  80 m   3 60  w
1 40 w   2  60 m   3 80  w
1 40 m
------------------------------------------------------------------
1 60 m   2 40 w     3 80  m
                                3 80  w
1 40 w   2  80 m
1 40 w                     3 60  w
1 40 m   2  60 m
------------------------------
1 40 ? -> w
1.原理
依次选择原始输入样本矩阵中的每一个特征，根据其取值的不同划分若干子集，使该子集中的相应特征取相同的值，借以提高样本集合的信息量，即减少信息熵。当全部特征都被用尽时，即完成整棵决策树的构建。对于未知输出的待预测样本，根据特征的相似性，将其归属到相似程度最高的叶子集中，通过投票(分类)或者平均(回归)获得与之对应的预测结果。
2.优化
根据数据集划分前后信息熵的减少量，优先选择部分可使熵减最大的特征进行决策树的构建，提前结束划分过程。牺牲部分次要特征，换取简化模型的性能提升。
3.集合算法
按照某种规则，利用原始的输入样本作为基础，构建多棵体现不同样本子集或特征子集的决策树模型，以投票或平均的方式综合多个模型的预测结果，形成相对一般(泛化)的最终结果。
1)自助聚合
在全部n个样本中，以有放回抽样的方式，选取m个样本，构建一棵决策树模型，重复以上过程b次，得到b棵决策树模型。
2)随机森林
在自助聚合的基础上，构建单棵决策树的时候，不但随机选取m个样本，而且随机挑选p个特征，利用对样本和特征的双重泛化，进一步削弱特殊样本和特征对模型预测结果的影响。
3)正向激励
初始化时，给训练集中的每个样本分配相等的权重，构建第一棵决策树模型，然后用该模型对训练集进行预测，针对预测错误的样本提升其权重，再构建第二棵决策树模型，依次类推，最终得到b棵样本权重各不相同的决策树模型。利用权重的差别，均化部分特殊样本对预测结果的影响。
import sklearn.tree as st
st.DecisionTreeRegressor() \ 单棵决策树
st.DecisionTreeClassifier()   /
import sklearn.ensemble as se
se.AdaBoostRegressor() \ 正向激励
se.AdaBoostClassifier()   /
se.RandomForestRegressor() \ 随机森林
se.RandomForestClassifier()   /
代码：house.py
4.特征重要性
所有基于决策树的学习模型，在确定划分子集所用特征时，都会遵循信息熵减少量最大化原则，由此可以得知不同特征的重要性指标：决策树模型对象.feature_importances_
超参数：事先人为给定参数，如正则强度、评估器数等等。
模型参数：决定预测算法的参数，如线性回归中的斜率和截距。
学习参数：在模型学习训练数据的过程中，额外提供的一些中间参数，如特征重要性等。
特征重要性一方面与模型使用的算法有关，另一方面还与训练数据的粒度有关。
代码：fi.py
共享单车需求数据分析
代码：bike.py
七、简单分类
x1 x2 y
3   1   0
2   5   1
1   8   1
6   4   0
5   2   0
3   5   1
4   7   1
4  -1  0
--------
8   2   ?->0
4   9   ?->1
x1>x2->y=0
x1<x2->y=1
代码：simple.py
------------------------------------------------------------------




























