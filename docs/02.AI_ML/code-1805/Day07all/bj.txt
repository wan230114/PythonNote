机器学习
一、概述
1.什么是机器学习
机器学习是一门能够让编程计算机从数据中学习的计算机科学。
机器学习研究如何让计算机不需要明确的程序也能具备学习能力。
一个计算机程序在完成某种任务之后，获得经验，该经验可以通过一个量化指标加以描述，如果该经验可以随着任务的丰富而不断提升，那么这样的计算机系统就被称为机器学习系统。
2.为什么需要机器学习
1)模式识别的过程可以自动完成，自动更新，易于维护，而且更加准确。
2)针对那些使用传统方法太过复杂，甚至根本不存在已知算法的问题。
3)帮助人类发现隐藏在数据中的规则――数据挖掘。
3.机器学习系统的种类
1)有监督学习、无监督学习、半监督学习和强化学习
输入  输出
a       1
b      1
c      0
d     1
e     0
f      1
.
.
.
x     ?
f(邮件)->是否垃圾
2)批量学习和在线学习
3)基于实例的学习和基于模型的学习
4.机器学习的挑战
1)训练数据不足
2)训练数据不具代表性
3)训练数据质量不佳
4)混在无关特征
5)过拟合和欠拟合
5.测试和验证
数据集--训练集
           \ 验证集
           \ 测试集
6.机器学习和人工智能
人工智能
       机器学习
              传统机器学习：基于标准算法，中小数据量
              深度学习：基于深度神经网络，海量数据量
       ...
7.机器学习的标准流程
1)数据采集
2)数据清洗
3)数据预处理
4)特征提取
5)选择模型(算法)
6)训练模型 <-+
7)验证模型 ---+
8)测试模型
9)使用模型
二、数据预处理
sci-kit learn
sklearn
nltk
opencv
nerolib
...
输入样本矩阵：一行一样本，一列一特征。
姓名 年龄 学历 -> 薪资
张三 25   本科       8000
李四 30   硕士       9000
王五 23   大专       10000
...
输出标签数组：对应于输入样本矩阵中的每个样本
[8000 9000 10000 ...]
1.均值移除
把输入样本矩阵的每个特征的平均值置为0，标准差置为1。
a
b
c
m = (a+b+c)/3
a' = a - m
b' = b - m
c' = c - m
m' = (a'+b'+c')/3
     = ((a-m)+(b-m)+(c-m))/3
     = ((a+b+c)-3m) / 3
     = 0
a'
b'
c'
s'^2 = (a'^2+b'^2+c'^2) / 3
a" = a'/s'
b" = b'/s'
c" = c'/s'
s"^2 = (a"^2+b"^2+c"^2)/3
          = (a'^2+b'^2+c'^2)/s'^2/3
          = 1
import sklearn.preprocessing as sp
sp.scale(原始样本矩阵)->均值移除后的样本矩阵
代码:std.py
2.范围缩放
        一班  二班 ...
数学   90     60
英语
...
将输入样本矩阵中的每个特征线性缩放到统一的最值区间。
x k + b = y
min k + b = 0
max k + b = 1
范围缩放器 = sp.MinMaxScaler(
    feature_range=(目标最小值, 目标最大值))
范围缩放器.fit_transform(原始样本矩阵)
    ->范围缩放后的样本矩阵
代码：mms.py
3.归一化
          Python C++ Java PHP
2015 10          20    30    15
2016 20          15    15    15
2017 ...
用每一个输入样本中的特征值除以该样本所有特征值的绝对值之和。
sp.normalize(原始样本矩阵, norm='l1')
    ->归一化后的样本矩阵
代码:nor.py
4.二值化
将输入样本矩阵中大于某个给定阈值的元素设定为1,其它元素均为0.
100 90 70 80 40 50 60
60
1 1 1 1 0 0 0
二值化器 = sp.Binarizer(threshold=阈值)
二值化器.transform(原始样本矩阵)
    ->二值化后的样本矩阵
代码:bin.py
5.独热编码
1            3            2
7            5            4
1            8            6
7            3            9
1: 10     3:100     2:1000
7: 01     5:010     4:0100
              8:001     6:0010
                             9:0001
101001000
010100100
100010010
011000001
独热编码器 = sp.OneHotEncoder(
    sparse=[是]否紧凑, dtype=类型)
独热编码器.fit_transform(原始样本矩阵)
    ->独热编码后的样本矩阵
代码:ohe.py
6.标签编码
按照字典顺序对每个特征中的文本标签进行编码,变成整数序列.
标签编码器 = sp.LabelEncoder()
标签编码器.fit_transform(标签序列)->编码序列
标签编码器.inverse_transform(编码序列)->标签序列
代码:lab.py
三、线性回归
x -> y
x -> y
x -> y
.
.
.
x -> ?
预测函数: y' = w0 + w1x
                           SIGMA[(y-(w0+w1x))^2]
损失函数: loss = -----------------------------
                                                 2
寻找合适的模型参数w0和w1使得损失函数的值最小.
dloss
------ = SIGMA[2(y-(w0+w1x))(-1)/2]
dw0   = -SIGMA[y-(w0+w1x)]
dloss
------ = SIGMA[2(y-(w0+w1x))(-x)/2]
dw1  = -SIGMA[(y-(w0+w1x))x]
w0 = w0 - ndloss/dwo
w1 = w1 - ndloss/dw1
代码:bgd.py
x1 x2 x3 ... xn -> y
y = w0 + w1x1 + w2x2 + w3x3 + ... + wnxn
import sklearn.linear_model as lm
线性回归器 = lm.LinearRegression()
线性回归器.fit(输入样本矩阵, 输出标签数组)
线性回归器.predict(输入样本矩阵)->预测的输出标签数组
------------------------------------------------------------------
代码：line.py
保存和载入模型
import pickle
pickle.dump(内存对象, 文件对象)
pickle.load(文件对象)->内存对象
代码：dump.py、load.py
四、岭回归
loss' = loss(w0, w1) - 正则强度 x 正则项(x, y)
岭回归就是在线性回归的基础上增加了正则项，有意破坏模型对训练数据集的拟合效果，客观上降低了少数异常样本对模型的牵制作用，使得模型对大多数正常样本表现出更好的拟合效果。
岭回归器 = lm.Ridge(alpha = 正则强度)
代码：rdg.py
五、多项式回归
y' = w0 + w1x + w2x^2 + w3x^3 + ... + wnx^n
x1 = x
x2 = x^2
x3 = x^3
...
xn = x^n
y' = w0 + w1x1 + w2x2 + w3x3 + ... + wnxn
x->多项式特征扩展器-x1...xn->线性回归器->w0...wn
       \____________________________________/
                                    |
                         管线(流水线)
代码：poly.py
六、决策树
回归问题：输出标签分布于无限连续域。
分类问题：输出标签分布于有限离散域。
核心思想：相似的因导致相似的果。
驾龄    均速   ->   性别
1         60             m
2         40             w
3         80             m
1         40             w
2         80             m
1         40             w
1         40             m
3         60             w
2         60             m
3         80             w
------------------------------------------------------------------
1 60 m   2 40 w    3 80  m
1 40 w   2  80 m   3 60  w
1 40 w   2  60 m   3 80  w
1 40 m
------------------------------------------------------------------
1 60 m   2 40 w     3 80  m
                                3 80  w
1 40 w   2  80 m
1 40 w                     3 60  w
1 40 m   2  60 m
------------------------------
1 40 ? -> w
1.原理
依次选择原始输入样本矩阵中的每一个特征，根据其取值的不同划分若干子集，使该子集中的相应特征取相同的值，借以提高样本集合的信息量，即减少信息熵。当全部特征都被用尽时，即完成整棵决策树的构建。对于未知输出的待预测样本，根据特征的相似性，将其归属到相似程度最高的叶子集中，通过投票(分类)或者平均(回归)获得与之对应的预测结果。
2.优化
根据数据集划分前后信息熵的减少量，优先选择部分可使熵减最大的特征进行决策树的构建，提前结束划分过程。牺牲部分次要特征，换取简化模型的性能提升。
3.集合算法
按照某种规则，利用原始的输入样本作为基础，构建多棵体现不同样本子集或特征子集的决策树模型，以投票或平均的方式综合多个模型的预测结果，形成相对一般(泛化)的最终结果。
1)自助聚合
在全部n个样本中，以有放回抽样的方式，选取m个样本，构建一棵决策树模型，重复以上过程b次，得到b棵决策树模型。
2)随机森林
在自助聚合的基础上，构建单棵决策树的时候，不但随机选取m个样本，而且随机挑选p个特征，利用对样本和特征的双重泛化，进一步削弱特殊样本和特征对模型预测结果的影响。
3)正向激励
初始化时，给训练集中的每个样本分配相等的权重，构建第一棵决策树模型，然后用该模型对训练集进行预测，针对预测错误的样本提升其权重，再构建第二棵决策树模型，依次类推，最终得到b棵样本权重各不相同的决策树模型。利用权重的差别，均化部分特殊样本对预测结果的影响。
import sklearn.tree as st
st.DecisionTreeRegressor() \ 单棵决策树
st.DecisionTreeClassifier()   /
import sklearn.ensemble as se
se.AdaBoostRegressor() \ 正向激励
se.AdaBoostClassifier()   /
se.RandomForestRegressor() \ 随机森林
se.RandomForestClassifier()   /
代码：house.py
4.特征重要性
所有基于决策树的学习模型，在确定划分子集所用特征时，都会遵循信息熵减少量最大化原则，由此可以得知不同特征的重要性指标：决策树模型对象.feature_importances_
超参数：事先人为给定参数，如正则强度、评估器数等等。
模型参数：决定预测算法的参数，如线性回归中的斜率和截距。
学习参数：在模型学习训练数据的过程中，额外提供的一些中间参数，如特征重要性等。
特征重要性一方面与模型使用的算法有关，另一方面还与训练数据的粒度有关。
代码：fi.py
共享单车需求数据分析
代码：bike.py
七、简单分类
x1 x2 y
3   1   0
2   5   1
1   8   1
6   4   0
5   2   0
3   5   1
4   7   1
4  -1  0
--------
8   2   ?->0
4   9   ?->1
x1>x2->y=0
x1<x2->y=1
代码：simple.py
------------------------------------------------------------------
八、逻辑分类
                               1
y' = --------------------------------- -> 归属1类别的概率
        1 + e^-(w0 + w1x1 + w2x2)
                                       0     1
3   1 -> 0.1 (<0.5) 0   0.9  0.1
2   5 -> 0.7 (>0.5) 1   0.3  0.7
逻辑分类器 = LogisticRegression()
代码：log2.py
                      A          B          C
3   1   A  1   0.8   0  0.2   0  0.4   A
2   5   B  0   0.4   1  0.6   0  0.1   B
1   8   C  0   0.3   0  0.4   1  0.9   C
代码：log3.py
九、朴素贝叶斯分类
1.原理
3   1   甲
2   5   乙
1   8   丙
...
x1 x2 C
\___/
   X
计算待预测样本X隶属于C类别的概率：P(C|X)
贝叶斯定理：P(A|B)=P(A)P(B|A)/P(B)
                     P(B)P(A|B)=P(A)P(B|A)
                     P(B, A)=P(A, B)
100
10
1
P(C|X)=P(C)P(X|C)/P(X)
P(C)P(X|C)
=P(C, X)
=P(C, x1, x2)
=P(x1, x2, C)
=P(x1|x2, C)P(x2, C)
=P(x1|x2, C)P(x2|C)P(C)
朴素：条件独立，所有特征值之间无关联性。
=P(x1|C)P(x2|C)P(C)
...
=P(x1|C)P(x2|C)P(x3|C)...P(xn|C)P(C)
import sklearn.naive_bayes as nb
朴素贝叶斯分类器 = nb.GaussianNB()
代码：nb.py
2.划分测试集和训练集
import sklearn.model_selection as ms
ms.train_test_split(输入集, 输出集, 测试集比例, 随机种子)
    -> 训练输入集, 测试输入集, 训练输出集, 测试输出集
用训练输入集和输出集训练模型，然后用测试输入集预测输出，再用预测输出集和测试输出集评估模型的性能。
                      测试输入集
                              |
训练输入集 \          v
                    > 模型对象
训练输出集 /          |
                              v
                     预测输出集    测试输出集
                      \_____________________/
                                       |
                                       v
                                 评估指标
代码：split.py
3.查准率、召回率和F1得分
查准率，正确性，对：找对的/找出来的
召回率，完整性，全：找对的/实际有的
                2 x 查准率 x 召回率
F1得分 = ---------------------
                   查准率 + 召回率
4.交叉验证
ms.cross_val_score(模型对象, 输入集, 输出集, cv=次数
    scoring=指标名称)->得分数组
指标名称：precision_weighted，查准率
                  recall_weighted，召回率
                  f1_weighted，F1得分
                  accuracy，正确率
代码：cv.py
5.混淆矩阵
              预测类0 预测类1 预测类2 预测类3
实际类0 100       0            0           0
实际类1  90        10          0           0
实际类2
实际类3
sm.confusion_matrix(实际类别标签, 预测类别标签)
    -> 混淆矩阵
代码：cm.py
6.分类报告
sm.classification_report(实际类别标签, 预测类别标签)
    -> 分类报告
代码：cr.py
十、随机森林分类
1.汽车质量评估
代码：car.py
2.验证曲线
验证曲线表现的是模型超参数的不同取值和模型性能之间的函数关系。
ms.validation_curve(
    模型, 输入集, 输出集, 超参数名, 取值列表, cv=验证次数)
    -> 训练集f1得分矩阵，测试集f1得分矩阵
          次数1 次数2 次数3 ...
取值1 0.9     0.8    0.9
取值2 0.8     0.7    0.6
取值3 0.6     0.9    0.7 
...
代码：vc.py
3.学习曲线
学习曲线表现的是训练集大小的不同取值和模型性能之间的函数关系。
ms.learning_curve(
    模型, 输入集, 输出集, 训练集大小列表, cv=验证次数)
    -> 训练集大小列表, 训练集f1得分矩阵，测试集f1得分矩阵
                        次数1 次数2 次数3 ...
训练集样本数1 0.9     0.8    0.9
训练集样本数2 0.8     0.7    0.6
训练集样本数3 0.6     0.9    0.7 
...
代码：lc.py
十一、支持向量机(SVM)
1.分类目标
1)寻找合理的分类边界，使得所有类别关于该边界线的支持向量与分类边界的间距相等且最大。
2)分类边界必须线性（一次方程）。
3)对于无法线性分割的样本，通过核函数将其升起高维度空间，寻求线性分类边界。
超平面
2.线性核函数(不做升维变换)
代码：svm_line.py
3.多项式核函数(用幂函数升维变换)
代码：svm_poly.py
4.径向基核函数(用高斯分布函数升维变换)
代码：svm_rbf.py
5.类型失衡问题
在分类过程中，那些样本比例明显偏多的类别更有机会被分类器作为预测结果，从而导致较大的误差。支持向量机分类器通过为不同比例的类别分配不同的权重，来平衡它们在分类过程中的作用，尽可能做到公平。
svm.SVC(..., class_weight='balanced', ...)
代码：svm_bal.py
6.置信概率
支持向量机分类器是依据分类边界线的位置计算每个样本的置信概率的，那些距离边界线较近的样本，置信概率就较低，反之较高。
model = svm.SVC(..., probability=True, ...)
model.predict_proba(输入集)
    -> 置信概率矩阵
                  类别1 类别2 类别3 ...
输入样本1  0.5     0.3    0.1      1
输入样本2  0.2     0.4    0.5      3 
输入样本3  0.1     0.3    0.2      2 
...
代码：svm_prob.py
7.网格搜索
针对不同超参数的组合，进行多次交叉验证，从中选择性能指标最好的参数组合来构建模型对象。
ms.GridSearchCV(模型对象, 超参数列表, cv=验证次数)
     ->最优模型对象
代码：svm_grid.py
8.事件预测案例
代码：svm_evt.py
xxxx -> 12.5
xxxx -> 10.2
xxxx -> 19.3
xxxx -> 11.7
...
9.交通流量预测案例
代码：svm_trf.py
十二、聚类
1.K均值算法
欧几里得距离：
一维空间：x1, x2 -> |x1-x2|, sqrt((x1-x2)^2)
二维空间：(x1,y1), (x2,y2) -> sqrt((x1-x2)^2+(y1-y2)^2)
三维空间：(x1,y1,z1), (x2,y2,z2) ->
                              sqrt((x1-x2)^2+(y1-y2)^2+(z1-z2)^2)
N维空间：(x1,x2,...,xn), (x1',x2',...,xn') ->
                    sqrt((x1-x1')^2+(x2-x2')^2+...+(xn-xn')^2)
第一步：随机选择k个样本作为聚类中心；
第二步：遍历除中心以外的样本，计算它们与每个聚类中心的欧氏距离，选择距离最近中心的类别作为该样本的类别，完成一次聚类划分；
第三步：针对每一个已划分好的聚类，计算它们的几何中心，将该几何中心作为新的聚类中心，重复第二步，再次进行聚类划分，直到某次聚类划分之后所得到的几何中心与所依据的聚类中心重合或足够接近为止。
代码：km.py
利用K均值聚类量化图像
代码：quant.py
2.均值漂移算法
将待划分聚类的样本集合看作服从某种概率规则的随机分布，通过多次迭代，找到与样本分布直方图具有最佳拟合形态的概率模式，即最优概率密度函数，该函数的峰值位置即聚类中心，而后再根据每个样本距离聚类中心的距离远近划分不同的聚类。
代码：shift.py
3.凝聚层次算法
首先将每个样本看作是独立的集群，如果集群数大于事先的给定值，则根据距离远近由每个样本出发凝聚与之最近的样本形成新的更大的集群，不断扩大集群的规模，同时缩减集群的数量，直到达到给定值为止，这样得到的每个集群即最终划分的聚类。
代码：agglo.py
出来根据距离以外还可以根据连续性来选择被凝聚的样本。
代码：spiral.py
4.评价指标
评价一个聚类模型的性能，关注它是否体现了内密外疏的原则。
一个样本的轮廓系数 = (b - a) / max(a, b)
a: 该样本与同聚类其它样本的平均距离。
b: 该样本与和它距离最近的另一个聚类中各样本的平均距离。
整个模型的轮廓系数 = 所有样本的轮廓系数的算数平均值
-1 <- 0 -> 1
差    重叠  优
sm.silhouette_score(
    输入集, 预测类别, 样本数, metric=距离种类)->轮廓系数
代码：score.py
5.DBSCAN(噪声密度)算法
从待聚类样本中随机选择一个样本，以预先给定的"半径"做"圆"，所有被该圆圈中的的样本被视为与所选样本同类，在以这些被圈中样本为中心做圆重复以上过程，不断增加聚类的成员，直到没有新的成员加入为止，即完成了一个聚类的划分。而后，再从其余样本中随机选择一个重复以上过程，直至耗尽所有样本为止。
外周样本：位于某个样本的扩展范围内，但却无法扩展出新的成员。
孤立样本：既不能为任何样本扩展为聚类成员，同时也无法引入其它成员组成聚类。
核心样本：除了外周样本和孤立样本以外的都属于核心样本。
代码：dbscan.py
十三、流水管线
1.map：Python语言对标量操作矢量化的支持。
def 标量函数(标量参数):
    ...
    return 标量返回值
map(标量函数, 矢量)
    ->由标量函数的标量返回值组成的矢量返回值迭代器
代码：map.py
2.reduce: functools提供的一种针对矢量的降维操作。
一维 --降维->零维
矢量               标量
[2, 3, 4] -> 9, 策略：累加
[2, 3, 4] -> 24,  策略：累乘
def 策略函数(参数1, 参数2):
    返回值 = 参数1 + 参数2
    return 返回值
functools.reduce(策略函数, 矢量)->标量(D)
矢量：[a, b, c, d, e]
策略函数(a, b)->A
策略函数(A, c)->B
策略函数(B, d)->C
策略函数(C, e)->D
代码：reduce.py
3.函数级联：用一个函数调用的返回值作为另一个函数的参数。
def f1(arg):
    ...
    return rtv
def f2(arg):
    ...
    return rtv
def f3(arg):
    ...
    return rtv
a = 10
b = f1(a)
c = f2(b)
d = f3(c)
d = f3(f2(f1(a)))
a->f1->f2->f3->d
代码：cc1.py、cc2.py
4.管线对象
make_pipeline
管线对象 = Pipeline([
    (部件名1, 部件对象1),
    (部件名2, 部件对象2),
    (部件名3, 部件对象3),
    ...])
管线对象.fit(训练数据)
训练数据
    ->部件对象1.fit_transform()
        ->部件对象2.fit_transform()
            ->部件对象3.fit()
管线对象.predict(测试输入)
测试输入
    ->部件对象1.fit_transform()
        ->部件对象2.fit_transform()
            ->部件对象3.predict()
                ->测试输出
x->多项式特征扩展器-x,x^2,x^3,...,x^n->线性回归器
x1,x2,...,xn->特征优选器->x1,x5,x9->随机森林分类器
                      SelectKBest              RandomForestClassifier
管线对象.named_steps['部件名'] -> 部件对象
管线对象.set/get_params(部件名__超参数名[=值])[->值]
                                                      ^
                                                       |
                                                 双下划线
代码：pipe.py
十四、寻找最近邻
1.FNN
import sklearn.neighbors as sn
model = sn.NearestNeighbors(
    n_neighbors=近邻数, algorithm=优化算法)
优化算法：ball_tree
model.fit(训练样本集合)
model.kneighbors(测试样本集合)->
    近邻矩阵、距离矩阵
                 近邻1        近邻2        近邻3
测试样本1 训练样本6 训练样本8 训练样本19
测试样本2
测试样本3
...
                 近邻1        近邻2        近邻3
测试样本1 0.8           1.2            5.4
测试样本2
测试样本3
...
代码：fnn.py
2.KNN回归和分类
计算待预测样本与训练样本集中每个样本的距离，找出最近的K个邻居，待预测样本所对应的输出由这K个邻居通过加权(与距离成反比)投票(分类)或平均(回归)产生。
分类，不同类别样本的比例尽量均衡。
回归，只能预测训练数据范围之内或者附近的未知样本。
惰性学习方法，训练阶段不会识别任何模式，速度很快，但是所有的预测工作都需要涉及大量的距离计算，耗时很长，因此不适于处理规模过于巨大的训练数据集，必须在时间性能和精度性能之前权衡。
代码：knnc.py、knnr.py
十五、推荐引擎
1.欧氏距离得分
                                    1
欧氏距离得分 = --------------
                          1 + 欧氏距离
     0 <--> 1
不相似     相似
    A    B    C    D    E    F    G
A 1   0.7 0.5 0.9 0.2 0.4 0.3
B
C
D
E
F
G
代码：es.py
2.皮氏距离得分
相关系数
/                \
|  1    0.35 |
|  0.35    1 |
\                /
  -1 <-> 0 <-> 1
相反     无关    相同
代码：ps.py
3.按照相似度从高到低的顺序，列出每个用户的相似用户
代码：sim.py
4.生成推荐清单
   0.9  0.7 0.8 -1
   *     *     *  *
   B C D E F G    xxx
A 2    3    4  5 
0.9x2+0.7x3+0.8x4
----------------------- = xxx对A的推荐力度
     0.9+0.7+0.8
代码：rcm.py
十六、自然语言
声音->文本->语义->逻辑处理->语义->文本->声音
\_______/\_____/\________________/\______/\______/
语音处理 自然语言   业务逻辑         自然语言 语音合成
NLTK - 自然语言工具包
1.分词
将复杂的文本拆分成语义相对独立且简单的单元，如句子、词组或者单词。
import nltk.tokenize as tk
tk.sent_tokenize(文章)->句子列表
tk.word_tokenize(句子)->单词列表
分词器对象 = tk.WordPunctTokenizer()
分词器对象.tokenize(句子)->单词列表
代码：tkn.py
2.词干提取
从单词中抽取与语义联系最为紧密，且变化较少的部分，有意丢弃那些与数、词性、时态、语态等有关的次要部分。算法本身并不保证所提取的词干依然是合法的单词。
import nltk.stem.porter as pt
波特词干提取器 = pt.PorterStemmer() # 宽松
import nltk.stem.lancaster as lc
郎卡斯特词干提取器 = lc.LancasterStemmer() # 严格
import nltk.stem.snowball as sb
斯诺伯词干提取器 = sb.SnowballStemmer(语言) # 中间
xxx词干提取器.stem(单词)->词干
代码：stm.py
3.词形还原
对于名词：复数->单数
对于动词：分词->原型
import nltk.stem as ns
词形还原器 = ns.WordNetLemmatizer()
词形还原器.lemmatize(单词, pos=词性)->单数/原型
代码：lmm.py
4.词袋(BOW, Bag of Words)模型
The brown dog is running.
The black dog is in the black room.
Running in the room is forbidden.
-----------------------------------------------------------
   the brown dog is running black in room forbidden
1 1    1           1     1   1            0        0  0        0
2 2    0           1     1   0            2        1  1        0
3 1    0           0     1   1            0        1  1        1
import sklearn.feature_extraction.text as ft
词袋特征抽取器 = ft.CountVectorizer()
词袋特征抽取器.fit_transform(句子列表)->词袋矩阵
代码：bow.py
5.词频矩阵
真正表示单词和句子语义关联性的并非该单词在句子中出现的次数，而是它所占的比例，因此用词袋矩阵的归一化矩阵表示文本序列的特征更加有利，这样的特征矩阵谓之词频矩阵。
代码：tf.py
6.文档频率
                                               包含该单词的样本数
词表中某个单词的文档频率 = ---------------------
                                                       总样本数
文档频率越低的单词，对于区分样本差别的贡献越大。
7.TF-IDF，词频逆文档频率
词频矩阵中的每个元素乘以相应单词文档频率的倒数。
词频逆文档频率抽取器 = ft.TfidfTransformer()
词频逆文档频率抽取器.fit_transform(词袋矩阵)
    ->词频逆文档频率矩阵
代码：tfidf.py
8.文本分类
代码：doc.py
1 2 3 4 5 6
5 0 2 0 3 0
0 4 1 0 0 5
...
9.性别识别
姓名 性别 年龄 -> 收入
张三 男    22         5000
李四 女    20         5500
...
[
({姓名: 张三,  性别: 男, 年龄: 22}, 5000),
({姓名: 李四,  性别: 女, 年龄: 20}, 5500),
...]

代码：gndr.py
10.情感分析
               apple banana lime
fileid -> True   True     True 
fileid -> False   True    False
...
代码：sent.py
十七、语音识别
1.音频
声音的本质就是震动，因此对声音的描述就是位移关于时间的函数。通过傅里叶变换，可以把这个位移关于时间函数映射为能量关于频率的函数，进而从声音的频谱中挖掘特征，用于机器学习。
代码：audio.py
2.识别
结合语音的特点，将每个单词的频谱作为输入，单词作为输出，建立一个分类器，识别出频谱和单词之间的联系，对未知输入的频谱，预测与之对应的输出，即完成语音识别。
频谱1 -> 单词1
频谱2 -> 单词2
...
3.梅尔频率倒谱系数(MFCC)
代码：mfcc.py
4.单词识别
代码：spch.py
十八、图像识别
1.OpenCV基础
机器视觉
代码：basic.py
2.边缘检测
代码：edge.py
3.亮度提升
代码：eq.py
4.角点检测
代码：harris.py
5.关键点检测
代码：star.py、sift.py
6.图像特征值(描述)矩阵
图像像素集合->STAR关键点检测器->关键点
                                                                |
                                                                v
              图像特征值(描述)矩阵<-SIFT关键点检测器
                              |
                              v
                   隐马尔科夫模型<--未知内容的图像 \
                              |                                             \ 相似度
                              v                                            / 比较
                        统计模式     -->反映图像的内容 /
代码：desc.py
7.图像识别
代码：obj.py
十九、人脸识别
1.视频捕捉
代码：vidcap.py
2.人脸定位
代码：haar.py
3.局部二值模式直方图(LBPH)人脸识别
代码：face.py
二十、神经网络
1.感知机
import neurolab as nl
代码：neuron.py
































